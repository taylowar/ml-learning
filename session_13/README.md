# Fixing backpropagation
* As of right now backpropagation is implementend is a slghtly untraditional fashion (look here grad.pdf) 
* 

# Sigmoid

# ReLu Activation function
$$
\begin{equation}
f(x) = 
\begin{cases}
    x & \text{if }~~ x > 0 \\
    0 & \text{otherwise.}
\end{cases}
\end{equation}$$
# Neural Framework Visual Trainer
* Train your neural networks GUI style, yeah

# Leaky ReLu ~ LReLu

# Tanh
* IT IS SO GOD DAMN FAST
    
# GeLu
* Like ReLu but smooth
* TODO: implement

# Sin
* Funky

---

* stb_image: [github](https://raw.githubusercontent.com/nothings/stb/master/stb_image.h)
* stb_image_write: [github](https://raw.githubusercontent.com/nothings/stb/master/stb_image_write.h)
* mnist images: [github](https://github.com/myleott/mnist_png/tree/master)
